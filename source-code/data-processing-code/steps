1--follow link

https://cloud.google.com/dataflow/docs/guides/templates/creating-templates#creating-and-staging-templates

2--create python pipline file with valueprovide , not add argument

parser.add_value_provider_argument(
    '--input',
    help='Input for the pipeline',
    default='gs://my-bucket/input'
)
It’s important to note that if you don’t use the ValueProvider for your runtime arguments, then Dataflow will not use those arguments when you try to provide them at runtime.

3 --run below command to create temlapte at gcp

python3 -m gcp_dataflow_pipeline --runner DataflowRunner --project gcp-learning-333002 --staging_location gs://gcp_etl_project/dataflow_templates/product_category/staging
--temp_location gs://gcp_etl_project/dataflow_templates/product_category/temp
--template_location gs://gcp_etl_project/dataflow_templates/product_category/template/GCP_Product_Category_ETL --region us-west1

4 ---creat3e metadata file to store as template name
   GCP_Product_Category_ETL_metadata

5 ----copy to the same localtion

6 --- create cloud function on new file arrival in gcp

def trigger_etl_flow(event, context):
    """Triggered by a change to a Cloud Storage bucket.
    Args:
         event (dict): Event payload.
         context (google.cloud.functions.Context): Metadata for the event.
    """
    file = event
    dataflow_exec(file,context)
    print(f"Processing file: {file['name']}.")

def dataflow_exec(file,context) :
	from googleapiclient.discovery import build
	#replace with your projectID
	project = "gcp-learning-333002"
	job = project + " olist_product_category_etl " + str(file['timeCreated'])
	#path of the dataflow template on google storage bucket
	template = "gs://gcp_etl_project/dataflow_templates/product_category/template/GCP_Product_Category_ETL"
	inputFile = "gs://" + str(file['bucket']) + "/" + str(file['name'])
	#user defined parameters to pass to the dataflow pipeline job
	parameters = {
		'input': inputFile,
	}
	#tempLocation is the path on GCS to store temp files generated during the dataflow job
	environment = {'tempLocation': 'gs://gcp_etl_project/dataflow_templates/product_category/temp'}

	service = build('dataflow', 'v1b3', cache_discovery=False)
	#below API is used when we want to pass the location of the dataflow job
	request = service.projects().locations().templates().launch(
		projectId=project,
		gcsPath=template,
		location='us-west1',
		body={
			'jobName': job,
			'parameters': parameters,
			'environment':environment
		},
	)
	response = request.execute()
	print(str(response))

	requirement.txt

	google-api-python-client
	setuptools
toml
pyproject.toml
wheel

